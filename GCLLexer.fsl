// The generated lexer module will start with this code
{
module GCLLexer
open FSharp.Text.Lexing
open System
// open the module that defines the tokens
open GCLParser
}

// We define macros for some regular expressions we will use later
let digit       = ['0'-'9']
let var         = ['A'-'Z' 'a'-'z'] (['A'-'Z' 'a'-'z' '_' ] | digit)*
let num         = digit+ ( '.' digit+)?  ('E' ('+'|'-')? digit+ )? 
let whitespace  = [' ' '\t'] 
let newline     = "\n\r" | '\n' | '\r' // We define now the rules for recognising and building tokens // for each of the tokens of our language we need a rule // NOTE: rules are applied in order top-down.
//       This is important when tokens overlap (not in this example)
rule tokenize = parse
// deal with tokens that need to be ignored (skip them)
| whitespace    { tokenize lexbuf }
| newline       { lexbuf.EndPos <- lexbuf.EndPos.NextLine; tokenize lexbuf; }
// deal with tokens that need to be built
| ":="          { ASS }
| "skip"        { SKIP }
| ';'           { SEMI }
| "if"          { IF }
| "fi"          { FI }
| "do"          { DO }
| "od"          { OD }
| "->"          { ARROW }
| "[]"          { IFELSE }
| "true"        { TRUE }
| "false"       { FALSE }
| '['           { LBRAC }
| ']'           { RBRAC }
| '*'           { TIMES }
| '/'           { DIV }
| '+'           { PLUS }
| '-'           { MINUS }
| '^'           { POW }
| '('           { LPAR }
| ')'           { RPAR }
| "sqrt"        { SQRT }
| '|'           { OR }
| "||"          { SCOR }   
| '&'           { AND }   
| "&&"          { SCAND }
| '!'           { NUT }
| '='           { EQUAL }
| "!="          { NEQ }
| '>'           { GT }
| '<'           { LT }
| ">="          { GEQ }
| "<="          { LEQ }
| "-D"          { DFLAG }
| "-ND"         { NDFLAG }
| "-P"          { PFLAG }
| "-SW"         { STEPFLAG }
| "-SA"         { SAFLAG }
| eof           { EOF }
| var           { VAR(LexBuffer<_>.LexemeString lexbuf) }
| num           { NUM(int (LexBuffer<_>.LexemeString lexbuf)) }
